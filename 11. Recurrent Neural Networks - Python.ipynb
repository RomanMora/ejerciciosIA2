{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "colab": {
      "name": "11. Recurrent Neural Networks - Python.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RomanMora/ejerciciosIA2/blob/main/11.%20Recurrent%20Neural%20Networks%20-%20Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "AnLE8RyCNrx-"
      },
      "source": [
        "Exercise 11 - Recurrent Neural Networks\n",
        "========\n",
        "\n",
        "A recurrent neural network (RNN) is a class of neural network that excels when your data can be treated as a sequence - such as text, music, speech recognition, connected handwriting, or data over a time period. \n",
        "\n",
        "RNN's can analyse or predict a word based on the previous words in a sentence - they allow a connection between previous information and current information.\n",
        "\n",
        "This exercise looks at implementing a LSTM RNN to generate new characters after learning from a large sample of text. LSTMs are a special type of RNN which dramatically improves the model’s ability to connect previous data to current data where there is a long gap.\n",
        "\n",
        "We will train an RNN model using a novel written by H. G. Wells - The Time Machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cDGKEauNryC"
      },
      "source": [
        "Step 1\n",
        "------\n",
        "\n",
        "Let's start by loading our libraries and text file. This might take a few minutes.\n",
        "\n",
        "#### Run the cell below to import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "M1gc7Q_VNryl"
      },
      "source": [
        "%%capture\n",
        "# Run this!\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, LSTM\n",
        "from keras.callbacks import LambdaCallback, ModelCheckpoint\n",
        "import numpy as np\n",
        "import random, sys, io, string"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvqN-cHfNrzY"
      },
      "source": [
        "#### Replace the `<addFileName>` with `The Time Machine`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "DEt5sAZwNrze",
        "outputId": "0c44d961-23e1-41c8-da7d-2d5441a71f61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "###\n",
        "# REPLACE THE <addFileName> BELOW WITH The Time Machine\n",
        "###\n",
        "text = io.open('The Time Machine.txt', encoding = 'UTF-8').read()\n",
        "###\n",
        "\n",
        "# Let's have a look at some of the text\n",
        "print(text[0:198])\n",
        "\n",
        "# This cuts out punctuation and make all the characters lower case\n",
        "text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "# Character index dictionary\n",
        "charset = sorted(list(set(text)))\n",
        "index_from_char = dict((c, i) for i, c in enumerate(charset))\n",
        "char_from_index = dict((i, c) for i, c in enumerate(charset))\n",
        "\n",
        "print('text length: %s characters' %len(text))\n",
        "print('unique characters: %s' %len(charset))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿The Time Traveller (for so it will be convenient to speak of him) was expounding a recondite matter to us. His pale grey eyes shone and twinkled, and his usually pale face was flushed and animated.\n",
            "text length: 174201 characters\n",
            "unique characters: 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCVoZJ4bNr0U"
      },
      "source": [
        "Expected output:  \n",
        "```The Time Traveller (for so it will be convenient to speak of him) was expounding a recondite matter to us. His pale grey eyes shone and twinkled, and his usually pale face was flushed and animated.\n",
        "text length: 174201 characters\n",
        "unique characters: 39```\n",
        "\n",
        "Step 2\n",
        "-----\n",
        "\n",
        "Next we'll divide the text into sequences of 40 characters.\n",
        "\n",
        "Then for each sequence we'll make a training set - the following character will be the correct output for the test set.\n",
        "\n",
        "### In the cell below replace:\n",
        "#### 1. `<sequenceLength>` with `40`\n",
        "#### 2. `<step>` with `4`\n",
        "#### and then __run the code__. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "SuGcGIU9Nr0X",
        "outputId": "813e73cc-d40e-4e29-f558-6fc1258d53b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "###\n",
        "# REPLACE <sequenceLength> WITH 40 AND <step> WITH 4\n",
        "###\n",
        "sequence_length = 40\n",
        "step = 4\n",
        "###\n",
        "\n",
        "sequences = []\n",
        "target_chars = []\n",
        "for i in range(0, len(text) - sequence_length, step):\n",
        "    sequences.append([text[i: i + sequence_length]])\n",
        "    target_chars.append(text[i + sequence_length])\n",
        "print('number of training sequences:', len(sequences))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training sequences: 43541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grLEl4yiNr02"
      },
      "source": [
        "Expected output:\n",
        "`number of training sequences: 43541`\n",
        "\n",
        "#### Replace `<addSequences>` with `sequences` and run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "kXRtdNOdNr05"
      },
      "source": [
        "# One-hot vectorise\n",
        "\n",
        "X = np.zeros((len(sequences), sequence_length, len(charset)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), len(charset)), dtype=np.bool)\n",
        "\n",
        "###\n",
        "# REPLACE THE <addSequences> BELOW WITH sequences\n",
        "###\n",
        "for n, sequence in enumerate(sequences):\n",
        "###\n",
        "    for m, character in enumerate(list(sequence[0])):\n",
        "        X[n, m, index_from_char[character]] = 1\n",
        "    y[n, index_from_char[target_chars[n]]] = 1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzO5UfubNr1k"
      },
      "source": [
        "Step 3\n",
        "------\n",
        "\n",
        "Let's build our model, using a single LSTM layer of 128 units. We'll keep the model simple for now, so that training does not take too long.\n",
        "\n",
        "### In the cell below replace:\n",
        "#### 1. `<addLSTM>` with `LSTM`\n",
        "#### 2. `<addLayerSize>` with `128`\n",
        "#### 3. `<addSoftmaxFunction>` with `'softmax`\n",
        "#### and then __run the code__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "zM7iHfxPNr1n"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "###\n",
        "# REPLACE THE <addLSTM> BELOW WITH LSTM (use uppercase) AND <addLayerSize> WITH 128\n",
        "###\n",
        "model.add(LSTM(128, input_shape = (X.shape[1], X.shape[2])))\n",
        "###\n",
        "\n",
        "###\n",
        "# REPLACE THE <addSoftmaxFunction> with 'softmax' (INCLUDING THE QUOTES)\n",
        "###\n",
        "model.add(Dense(y.shape[1], activation = 'softmax'))\n",
        "###\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhviGsBtNr16"
      },
      "source": [
        "The code below generates text at the end of an epoch (one training cycle). This allows us to see how the model is performing as it trains. If you're making a large neural network with a long training time it's useful to check in on the model as see if the text generating is legible as it trains, as overtraining may occur and the output of the model turn to nonsense.\n",
        "\n",
        "The code below will also save a model if it is the best performing model, so we can use it later.\n",
        "\n",
        "#### Run the code below, but don't change it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "EZJ1KT49Nr1-"
      },
      "source": [
        "# Run this, but do not edit.\n",
        "# It helps generate the text and save the model epochs.\n",
        "\n",
        "# Generate new text\n",
        "def on_epoch_end(epoch, _):\n",
        "    diversity = 0.5\n",
        "    print('\\n### Generating text with diversity %0.2f' %(diversity))\n",
        "\n",
        "    start = random.randint(0, len(text) - sequence_length - 1)\n",
        "    seed = text[start: start + sequence_length]\n",
        "    print('### Generating with seed: \"%s\"' %seed[:40])\n",
        "\n",
        "    output = seed[:40].lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    print(output, end = '')\n",
        "\n",
        "    for i in range(500):\n",
        "        x_pred = np.zeros((1, sequence_length, len(charset)))\n",
        "        for t, char in enumerate(output):\n",
        "            x_pred[0, t, index_from_char[char]] = 1.\n",
        "\n",
        "        predictions = model.predict(x_pred, verbose=0)[0]\n",
        "        exp_preds = np.exp(np.log(np.asarray(predictions).astype('float64')) / diversity)\n",
        "        next_index = np.argmax(np.random.multinomial(1, exp_preds / np.sum(exp_preds), 1))\n",
        "        next_char = char_from_index[next_index]\n",
        "\n",
        "        output = output[1:] + next_char\n",
        "\n",
        "        print(next_char, end = '')\n",
        "    print()\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "# Save the model\n",
        "checkpoint = ModelCheckpoint('Models/model-epoch-{epoch:02d}.hdf5', \n",
        "                             monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_lUvYDtNr2p"
      },
      "source": [
        "The code below will start the model to train. This may take a long time. Feel free to stop the training with the `square stop button` to the right of the `Run button` in the toolbar.\n",
        "\n",
        "Later in the exercise, we will load a pretrained model.\n",
        "\n",
        "### In the cell below replace:\n",
        "#### 1. `<addPrintCallback>` with `print_callback`\n",
        "#### 2. `<addCheckpoint>` with `checkpoint`\n",
        "#### and then __run the code__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "kb0dZPJMNr2w",
        "outputId": "71cb74f8-caf6-4aee-812c-ab802e0cc116",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "###\n",
        "# REPLACE <addPrintCallback> WITH print_callback AND <addCheckpoint> WITH checkpoint\n",
        "###\n",
        "model.fit(X, y, batch_size = 128, epochs = 3, callbacks = [print_callback, checkpoint])\n",
        "###"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "340/341 [============================>.] - ETA: 0s - loss: 2.7548\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \"ience of our time has attacked but a lit\"\n",
            "ience of our time has attacked but a lit fo i  i h ore  as tee rn ay thia an na aa  the  ie  ee en the the in g nd the t d st ihe eis  e sera in ih then i e  he  e we t c e d ie he ine  alald ane  ahe he aune mo ooe aas t ed othe tar le the he we the the to wer e y ant the s the tat a t ins the ai in  ame  y oao  he toe la ie  the aie te care te e cec aa e t io it the taher ue oml de nt ea fhan y le a he as  in  pe ann i in o o t ee rhe te e deina on ae it the aa tet tre  te e he s at  a tee the d e that  a  he a anut le as eoh s io e\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.75463, saving model to Models/model-epoch-01.hdf5\n",
            "341/341 [==============================] - 55s 161ms/step - loss: 2.7546\n",
            "Epoch 2/3\n",
            "340/341 [============================>.] - ETA: 0s - loss: 2.3882\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \" sat up in the freshness of the morning \"\n",
            " sat up in the freshness of the morning the the de the ang mes ind the seat ware ses was ind lime the and mas sare the the pas thing tti t the wes on thans tan the card at ind and the thet ion mom the t and in tore wast ant thed the the  on an tor and the hes wathe che the s an the the le lot con the the the s ald iol the rusth merind the s and ind me the ther pere the mes the me the tos and the pind ind that ind an the ay tout pe the and ter aun the wous mat the and want ine erit a the the ther the the sy faro the the men thr bet sar\n",
            "\n",
            "Epoch 00002: loss improved from 2.75463 to 2.38831, saving model to Models/model-epoch-02.hdf5\n",
            "341/341 [==============================] - 56s 164ms/step - loss: 2.3883\n",
            "Epoch 3/3\n",
            "341/341 [==============================] - ETA: 0s - loss: 2.2484\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \"you will find clues to it all’ then sudd\"\n",
            "you will find clues to it all’ then sudd wan wer an the sit the s awe the and the than the wat an the sargin the end wat that the mat be the the wit ind the and te the sore ind wat an the the sins the thin and als we the cat the thas and an ing we the the then bee sof the the game the the peamen an whe sand the be the mang and the th mang the seend the wer the the to the se trere ans an an the le sond of the pat il the the seren mas the the me the the sared wa le are on the the the the so warlebles the tore sout le and and in than the\n",
            "\n",
            "Epoch 00003: loss improved from 2.38831 to 2.24842, saving model to Models/model-epoch-03.hdf5\n",
            "341/341 [==============================] - 62s 183ms/step - loss: 2.2484\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f724a73eb70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNaytoh8Nr3A"
      },
      "source": [
        "The output won't appear to be very good. But then, this dataset is small, and we have trained it only for a short time using a rather small RNN. How might it look if we upscaled things?\n",
        "\n",
        "Step 5\n",
        "------\n",
        "\n",
        "We could improve our model by:\n",
        "* Having a larger training set.\n",
        "* Increasing the number of LSTM units.\n",
        "* Training it for longer\n",
        "* Experimenting with difference activation functions, optimization functions etc\n",
        "\n",
        "Training this would still take far too long on most computers to see good results - so we've trained a model already for you.\n",
        "\n",
        "This model uses a different dataset - a few of the King Arthur tales pasted together. The model used:\n",
        "* sequences of 50 characters\n",
        "* Two LSTM layers (512 units each)\n",
        "* A dropout of 0.5 after each LSTM layer\n",
        "* Only 30 epochs (we'd recomend 100-200)\n",
        "\n",
        "Let's try importing this model that has already been trained.\n",
        "\n",
        "#### Replace `<addLoadModel>` with `load_model` and run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "XwCW5Yo8Nr3E",
        "outputId": "9688f945-e443-4a78-cc65-4225ffaec38a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "print(\"loading model... \", end = '')\n",
        "\n",
        "###\n",
        "# REPLACE <addLoadModel> BELOW WITH load_model\n",
        "###\n",
        "model = load_model('arthur-model-epoch-30.hdf5')\n",
        "###\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam')\n",
        "###\n",
        "\n",
        "print(\"model loaded\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading model... model loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwP374taNr3a"
      },
      "source": [
        "Step 6\n",
        "-------\n",
        "\n",
        "Now let's use this model to generate some new text!\n",
        "\n",
        "#### Replace `<addFilePath>` with `'Data/Arthur tales.txt'`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "2XxR0c24Nr3d",
        "outputId": "bc74f02e-54b3-4e91-b3ba-07ddc1e8c743",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "###\n",
        "# REPLACE <addFilePath> BELOW WITH 'Data/Arthur tales.txt' (INCLUDING THE QUOTATION MARKS)\n",
        "###\n",
        "text = io.open('Arthur tales.txt', encoding='UTF-8').read()\n",
        "###\n",
        "\n",
        "# Cut out punctuation and make lower case\n",
        "text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "# Character index dictionary\n",
        "charset = sorted(list(set(text)))\n",
        "index_from_char = dict((c, i) for i, c in enumerate(charset))\n",
        "char_from_index = dict((i, c) for i, c in enumerate(charset))\n",
        "\n",
        "print('text length: %s characters' %len(text))\n",
        "print('unique characters: %s' %len(charset))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text length: 3645951 characters\n",
            "unique characters: 43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WXG-bMeNr4f"
      },
      "source": [
        "### In the cell below replace:\n",
        "#### 1. `<sequenceLength>` with `50`\n",
        "#### 2. `<writeSentence>` with a sentence of your own, at least 50 characters long.\n",
        "#### 3. `<numCharsToGenerate>` with the number of characters you want to generate (choose a large number, like 1500)\n",
        "#### and then __run the code__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "VyCm8SLfNr4i",
        "outputId": "d7344cc5-5048-43e6-cf41-fdf72bfc3743",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Generate text\n",
        "\n",
        "diversity = 0.5\n",
        "print('\\n### Generating text with diversity %0.2f' %(diversity))\n",
        "\n",
        "###\n",
        "# REPLACE <sequenceLength> BELOW WITH 50\n",
        "###\n",
        "sequence_length = 50\n",
        "###\n",
        "\n",
        "# Next we'll make a starting point for our text generator\n",
        "\n",
        "###\n",
        "# REPLACE <writeSentence> WITH A SENTENCE OF AT LEAST 50 CHARACTERS\n",
        "###\n",
        "seed = \"i have to much hungry because i did not have breakfast\"\n",
        "###\n",
        "\n",
        "seed = seed.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "###\n",
        "# OR, ALTERNATIVELY, UNCOMMENT THE FOLLOWING TWO LINES AND GRAB A RANDOM STRING FROM THE TEXT FILE\n",
        "###\n",
        "\n",
        "#start = random.randint(0, len(text) - sequence_length - 1)\n",
        "#seed = text[start: start + sequence_length]\n",
        "\n",
        "###\n",
        "\n",
        "print('### Generating with seed: \"%s\"' %seed[:40])\n",
        "\n",
        "output = seed[:sequence_length].lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "print(output, end = '')\n",
        "\n",
        "###\n",
        "# REPLACE THE <numCharsToGenerate> BELOW WITH THE NUMBER OF CHARACTERS WE WISH TO GENERATE, e.g. 1500\n",
        "###\n",
        "for i in range(800):\n",
        "###\n",
        "    x_pred = np.zeros((1, sequence_length, len(charset)))\n",
        "    for t, char in enumerate(output):\n",
        "        x_pred[0, t, index_from_char[char]] = 1.\n",
        "\n",
        "    predictions = model.predict(x_pred, verbose=0)[0]\n",
        "    exp_preds = np.exp(np.log(np.asarray(predictions).astype('float64')) / diversity)\n",
        "    next_index = np.argmax(np.random.multinomial(1, exp_preds / np.sum(exp_preds), 1))\n",
        "    next_char = char_from_index[next_index]\n",
        "\n",
        "    output = output[1:] + next_char\n",
        "\n",
        "    print(next_char, end = '')\n",
        "print()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \"i have to much hungry because i did not \"\n",
            "i have to much hungry because i did not have break and that is the boon that i may won the best knight of the round table i will put his sword for i will not slay thee and i will not have the fairest knight of the round table where is the most of the best knights of the world and i will be a great near more then sir launcelot rode forth and so he stood that the knight was ready to do with his armies and so they returned to the earth and then he slew him and asked him the head of the land of gold and greet and the day at the fall of his sword he said he was a service of it and the most part of the stones that he was a great while in the same wise  and the third they went to the castle and the maiden returned to the castle and there was a brother of silk and they set upon him and said behold the lord of this court for all this was a castle \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtsI2awMNr4z"
      },
      "source": [
        "How does it look? Does it seem intelligible?\n",
        "\n",
        "Conclusion\n",
        "--------\n",
        "\n",
        "We have trained an RNN that learns to predict characters based on a text sequence. We have trained a lightweight model from scratch, as well as imported a pre-trained model and generated new text from that."
      ]
    }
  ]
}